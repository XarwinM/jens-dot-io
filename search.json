[
  {
    "objectID": "impressum/index.html",
    "href": "impressum/index.html",
    "title": "Impressum",
    "section": "",
    "text": "Jens Müller\nHans-Thoma-Str. 8\n69121 Heidelberg\nE-Mail: jm (at) creal (punkt) de"
  },
  {
    "objectID": "impressum/index.html#kontakt-und-verantwortliche-person",
    "href": "impressum/index.html#kontakt-und-verantwortliche-person",
    "title": "Impressum",
    "section": "",
    "text": "Jens Müller\nHans-Thoma-Str. 8\n69121 Heidelberg\nE-Mail: jm (at) creal (punkt) de"
  },
  {
    "objectID": "blog/how_to_sample/index.html#greedy-search",
    "href": "blog/how_to_sample/index.html#greedy-search",
    "title": "LLM Sampling is Searching",
    "section": "Greedy Search",
    "text": "Greedy Search\nThe naive approach to text generation, known as greedy search, always selects the token with the highest probability at each step. While this seems intuitive, always choosing the most likely token at each step does not necessarily result in the most probable overall sequence.\n\nNo guarantee for global optimum\nUnlike single-output sampling methods—such as those used in diffusion models, generative adversarial networks (GANs), or normalizing flows—sequential sampling involves dynamically generating each token based on the context established by previous tokens.\nIn sequential sampling, the process can be visualized as navigating through a graph. Each node represents a token, and each directed edge between two nodes is weighted by \\(\\log p(x | c)\\), where \\(p(x | c)\\) is the conditional probability of the token \\(x\\) given the preceding context \\(c\\). The goal is to navigate this graph in a way that maximizes the total probability of the entire sequence1, calculated as the sum of the log-likelihoods of each token given its context. However, greedy search only considers the highest-probability edge at each step, potentially missing paths that yield a higher overall sequence probability.\n1 Every sequence must end with a specific end-of-sequence (EOS) token\n\nWhy to deviate?\nThere are two key reasons to deviate from the standard greedy search approach: First, it can reveal sequences with higher overall likelihood that might be overlooked by greedy decisions. Second, it enables finding sequences that align better with independent quality metrics, such as favoring unexpected or creative solutions."
  },
  {
    "objectID": "blog/how_to_sample/index.html#beam-search",
    "href": "blog/how_to_sample/index.html#beam-search",
    "title": "LLM Sampling is Searching",
    "section": "Beam Search",
    "text": "Beam Search\nBeam search expands the scope of exploration compared to greedy search by considering \\(k\\) different paths instead of just one. It evaluates the \\(k\\) most promising paths at each step. Beam search can be thought of as a more comprehensive version of greedy search, exploring multiple high-probability paths instead of committing to just one based on a local, next-token perspective.\nFor instance, consider a beam width of \\(k=3\\) and a vocabulary size of \\(128,000\\) tokens. Assume the three most likely sequences so far are \\(s_1\\), \\(s_2\\), and \\(s_3\\). At the next step, beam search computes probabilities for all possible next tokens for each of these sequences, leading to \\(3 \\cdot 128, 000 = 384,000\\) softmax outputs, corresponding to \\(p(x | s_i)\\) for \\(i = 1, 2, 3\\).\nAfter calculating these probabilities, beam search evaluates the scores for the entire set of possible continuations, selecting the top \\(k=3\\) outputs with the highest scores. The score for a candidate sequence is given by:\n\\[\n\\log p(x,s_i) = \\log p(x | s_i) + \\log p(s_i),\n\\]\nwhere \\(p(s_i)\\) represents the cumulative probability of the sequence \\(s_i\\) up to the current step. This scoring combines the likelihood of the continuation \\(\\log p(x | s_i)\\) and the likelihood of the path so far \\(\\log p(s_i)\\). The process repeats, always maintaining the \\(k\\) most promising sequences at each step, until a stopping condition is met."
  },
  {
    "objectID": "blog/how_to_sample/index.html#sampling-and-temperature-scaling",
    "href": "blog/how_to_sample/index.html#sampling-and-temperature-scaling",
    "title": "LLM Sampling is Searching",
    "section": "Sampling and Temperature Scaling",
    "text": "Sampling and Temperature Scaling\nInstead of always selecting the most likely token, we can sample tokens based on their predicted probabilities. Given a sequence of input tokens, an LLM computes logits, which are transformed into probabilities via a softmax layer. Since LLMs are trained using cross-entropy loss, these predicted probabilities aim to approximate the true probabilities observed in the training data 2.\n2 This statemeent is not entirely accurate, as LLMs are also trained using techniques from inverse reinforcement learning\nTemperature Scaling\nSampling from the predicted probability distribution versus always selecting the path with the locally highest likelihood (greedy search) can be understood as selecting between exploration and exploitation. Always taking the locally highest likelihood path corresponds to the greedy approach, where the LLM exclusively exploits its current knowledge. In contrast, sampling from the predicted probability distribution allows for exploration, enabling the LLM to consider alternative paths that may lead to better outcomes.\nTemperature scaling is a mechanism that controls the trade-off between exploitation (greedy search) and extreme exploration (random sampling). It achieves this by adjusting the sharpness of the probability distribution output by the LLM. The scaled probability distribution is given by:\n\\[\n\\text{softmax}(z, T) = \\frac{\\exp\\left(\\frac{z}{T}\\right)}{\\sum_i \\exp\\left(\\frac{z_i}{T}\\right)}\n\\]\n\nAt \\(T = 1\\), the original probability distribution predicted by the model is recovered without modification.\nAs \\(T\\) increases, the scaling reduces the influence of differences between probabilities. In the limit of high \\(T\\), all probabilities approach uniformity, leading to random sampling.\nAt \\(T = 0\\) , the model deterministically selects the token with the highest likelihood (greedy solution), as the probabilities become concentrated entirely on the most likely outcome.\n\nThus, temperature scaling provides a continuum between deterministic (greedy) selection and full random exploration, enabling fine-tuned control over the model’s behavior during sampling."
  },
  {
    "objectID": "blog/how_to_sample/index.html#keeping-the-explorer-in-line-top-k-top-p-min-p",
    "href": "blog/how_to_sample/index.html#keeping-the-explorer-in-line-top-k-top-p-min-p",
    "title": "LLM Sampling is Searching",
    "section": "Keeping the explorer in line: Top-\\(k\\), top-\\(p\\), min-\\(p\\)",
    "text": "Keeping the explorer in line: Top-\\(k\\), top-\\(p\\), min-\\(p\\)\nGiven the vast size of the token dictionary in an LLM, it is often efficient to focus only on the logits with the highest values. This approach offers two primary advantages:\n\nImproved Search Focus: By narrowing the selection to the most likely outputs, we reduce the risk of sampling highly unlikely tokens that could lead to incorrect results, effectively avoiding too explorative solutions.\nCost Efficiency: By pruning less relevant tokens, we minimize computational overhead, reducing overall processing costs.\n\nThis principle is implemented in top-\\(k\\) sampling, which selects only the top \\(k\\) tokens with the highest logits, discards the rest, and then computes the softmax probabilities for the remaining tokens to sample the next token.\nMin-\\(p\\) sampling operates similarly, but instead of selecting tokens based on logits, it computes the softmax probabilities first and retains only those tokens with a likelihood greater than a specified minimum threshold \\(p\\). Sampling is then restricted to this subset of tokens. While it may be take more compute than top-\\(k\\), min-\\(p\\) sampling can be more intuitive from a human perspective, as it relies on probabilities rather than logits, which do not inherently carry probabilistic meaning. For instance, all tokens generated in min-\\(p\\) sampling have at least probability \\(p\\) given the previous tokens. In top-\\(k\\) sampling we might encounter the scenario where only one sample carries almost all probability, but we consider \\(k-1\\) additional tokens during sampling.\nTop-\\(p\\) sampling (also known as nucleus sampling) takes a slightly different approach. It ensures that sampling is confined to the most probable tokens whose cumulative probabilities add up to at least \\(p\\). Let’s compare min-\\(p\\) sampling and top-\\(p\\) sampling with an example. Suppose we set \\(p = 0.2\\) for min-\\(p\\) sampling and \\(p = 0.8\\) for top-\\(p\\) sampling. It’s reasonable to use a higher \\(p\\) in top-\\(p\\) sampling since it defines not an individual threshold for each token but the cumulative probability weight of the distribution to sample from.\nNow consider two scenarios where one token dominates the probability distribution:\n\nIn the first scenario, the highest-probability token has a weight of 0.8, and all other tokens have probabilities below 0.2.\nIn the second scenario, the highest-probability token has a weight of 0.25, with the remaining tokens each having probabilities below 0.2.\n\nIn the first scenario, both min-\\(p\\) and top-\\(p\\) sampling would select only the highest-probability token for sampling, as it surpasses the individual threshold of \\(p = 0.2\\) (with no other token meeting this threshold) and satisfies the cumulative \\(p = 0.8\\) requirement.\nHowever, in the second scenario, the behavior diverges:\n\nMin-\\(p\\) sampling: Only the token with \\(p = 0.25\\) is considered, as all others fall below the threshold of 0.2. This rigid threshold excludes other tokens, potentially overlooking meaningful alternatives.\nTop-\\(p\\) sampling: The strategy adapts to the probability distribution by including the most likely tokens until their cumulative probability reaches 0.8. This ensures that multiple plausible tokens are considered, reflecting the underlying uncertainty in the distribution.\n\nThis adaptability makes top-\\(p\\) sampling more flexible and better suited for handling distributions where probabilities are spread among several tokens."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jens Müller",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\nHi, I am Jens. Here, I share topics I’m passionate about, ranging from mathematics and machine learning to futuristic speculations.\nA few words on my background and interests: I pursued and earned two master’s degrees — one in mathematics and the other in computer science. Beyond that, my curiosity led me to explore and study diverse fields, including psychology. During my time at Heidelberg University, I conducted research in machine learning, focusing on challenges related to distribution shifts. My work addressed broad questions like “How can we make reliable predictions when circumstances change?” (robustness, domain generalization) and “How can we detect when shifts and failures occur?” (OOD detection, failure detection, selective classification). I’ve published papers on these topics and completed my PhD in the process.\nCurrently, I work as a Data Scientist at Diamant Software, primarily focusing on Natural Language Processing (NLP) tasks."
  },
  {
    "objectID": "experiments/graph_viz.html",
    "href": "experiments/graph_viz.html",
    "title": "Package",
    "section": "",
    "text": "from openai import OpenAI\nfrom math import exp\nimport numpy as np\nfrom IPython.display import display, HTML\nimport os\n\nopen_ai_key = \"sk-None-BcvcTP9SBTw3yuuBzCE9T3BlbkFJq0ZmKf4zLrW2tpGZKAn5\"\n#open_ai_key = \"sk-proj-nfLesDiu6oVaARYzTSOBT3BlbkFJpW0Y8kdWyLMhQKyx86Ip\"\nos.environ[\"OPENAI_API_KEY\"] = open_ai_key \n\nclient = OpenAI()\n\n\ndef get_completion(\n    messages: list[dict[str, str]],\n    model: str = \"gpt-4o-mini\",\n    max_tokens=500,\n    temperature=0,\n    stop=None,\n    seed=123,\n    tools=None,\n    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n    top_logprobs=None,\n) -&gt; str:\n    params = {\n        \"model\": model,\n        \"messages\": messages,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"stop\": stop,\n        \"seed\": seed,\n        \"logprobs\": logprobs,\n        \"top_logprobs\": top_logprobs,\n    }\n    if tools:\n        params[\"tools\"] = tools\n\n    completion = client.chat.completions.create(**params)\n    return completion\n\n\nCLASSIFICATION_PROMPT = \"\"\"You will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: {headline}\"\"\"\n\n\nheadlines = [\n    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n]\n\n\nfor headline in headlines:\n    print(f\"\\nHeadline: {headline}\")\n    API_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n        model=\"gpt-4\",\n        logprobs=True,\n        top_logprobs=3,\n    )\n    top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n    html_content = \"\"\n    for i, logprob in enumerate(top_two_logprobs, start=1):\n        html_content += (\n            f\"&lt;span style='color: cyan'&gt;Output token {i}:&lt;/span&gt; {logprob.token}, \"\n            f\"&lt;span style='color: darkorange'&gt;logprobs:&lt;/span&gt; {logprob.logprob}, \"\n            f\"&lt;span style='color: magenta'&gt;linear probability:&lt;/span&gt; {np.round(np.exp(logprob.logprob)*100,2)}%&lt;br&gt;\"\n        )\n    display(HTML(html_content))\n    print(\"\\n\")\n\n\nHeadline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n\n\nOutput token 1: Technology, logprobs: -0.58375543, linear probability: 55.78%Output token 2: Explanation, logprobs: -1.008667, linear probability: 36.47%Output token 3: a, logprobs: -3.0114517, linear probability: 4.92%\n\n\n\n\n\nHeadline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n\n\nOutput token 1: Politics, logprobs: -0.0012809455, linear probability: 99.87%Output token 2: a, logprobs: -7.3205748, linear probability: 0.07%Output token 3: Explanation, logprobs: -7.4592924, linear probability: 0.06%\n\n\n\n\n\nHeadline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n\n\nOutput token 1: Art, logprobs: -0.09257728, linear probability: 91.16%Output token 2: Sports, logprobs: -2.4768393, linear probability: 8.4%Output token 3: A, logprobs: -5.93566, linear probability: 0.26%\n\n\n\n\n\n\n\nAPI_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n        model=\"gpt-4\",\n        logprobs=True,\n        top_logprobs=3,\n        max_tokens=1\n    )\n\n\ndef get_next_token(\n    messages: list[dict[str, str]],\n    model: str = \"gpt-4o-mini\",\n    temperature=0,\n    stop=None,\n    seed=123,\n    tools=None,\n    logprobs=None,\n    top_logprobs=None,\n) -&gt; str:\n    params = {\n        \"model\": model,\n        \"messages\": messages,\n        \"max_tokens\": 1,  # Limit to only the next token\n        \"temperature\": temperature,\n        \"stop\": stop,\n        \"seed\": seed,\n        \"logprobs\": logprobs,\n        \"top_logprobs\": top_logprobs,\n    }\n    if tools:\n        params[\"tools\"] = tools\n\n    completion = client.chat.completions.create(**params)\n    return completion.choices[0].message['content']\n\n\ndef generate_sequentially(\n    initial_messages: list[dict[str, str]],\n    model: str = \"gpt-4o-mini\",\n    max_tokens: int = 50,\n    temperature: float = 0,\n    stop=None,\n    seed=123,\n    tools=None,\n    logprobs=None,\n    top_logprobs=None,\n) -&gt; str:\n    # Copy the initial messages to maintain the history\n    messages = initial_messages.copy()\n    \n    # Initialize the assistant's message to collect generated tokens\n    current_response = \"\"\n    \n    for _ in range(max_tokens):\n        # Call get_next_token to get only the next token\n        next_token = get_next_token(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            stop=stop,\n            seed=seed,\n            tools=tools,\n            logprobs=logprobs,\n            top_logprobs=top_logprobs,\n        )\n        \n        # Check if the stop condition is met (like an &lt;EOS&gt; token)\n        if next_token == \"&lt;EOS&gt;\":\n            break\n        \n        # Append the next token to the current response\n        current_response += next_token\n        \n        # Update messages with the new token as part of the assistant's message\n        messages.append({\"role\": \"assistant\", \"content\": current_response})\n    \n    return current_response\n\n\nAPI_RESPONSE = generate_sequentially(\n        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n        model=\"gpt-4\",\n        logprobs=True,\n        top_logprobs=3,\n        max_tokens=2\n    )\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 API_RESPONSE = generate_sequentially(\n      2         [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n      3         model=\"gpt-4\",\n      4         logprobs=True,\n      5         top_logprobs=3,\n      6         max_tokens=2\n      7     )\n\nCell In[10], line 20, in generate_sequentially(initial_messages, model, max_tokens, temperature, stop, seed, tools, logprobs, top_logprobs)\n     16 current_response = \"\"\n     18 for _ in range(max_tokens):\n     19     # Call get_next_token to get only the next token\n---&gt; 20     next_token = get_next_token(\n     21         messages=messages,\n     22         model=model,\n     23         temperature=temperature,\n     24         stop=stop,\n     25         seed=seed,\n     26         tools=tools,\n     27         logprobs=logprobs,\n     28         top_logprobs=top_logprobs,\n     29     )\n     31     # Check if the stop condition is met (like an &lt;EOS&gt; token)\n     32     if next_token == \"&lt;EOS&gt;\":\n\nCell In[9], line 25, in get_next_token(messages, model, temperature, stop, seed, tools, logprobs, top_logprobs)\n     22     params[\"tools\"] = tools\n     24 completion = client.chat.completions.create(**params)\n---&gt; 25 return completion.choices[0].message['content']\n\nTypeError: 'ChatCompletionMessage' object is not subscriptable\n\n\n\n\n# parameters:\n** Model\n** sampling/ search\n*** Greedy, temperature, beam search, top-k, top-p\n** Sampling kewords\n** selection strategy\n*** Varentropy, Entropy \n\nThis package allows to explore an llm by showing the effect of different sampling and selection strategies.\n\nDemo\nmodel.predict( sampling=[‘top-k’, ‘blub’], ordering=‘heuristic’, n_samples=20, max_token=128 )\nm"
  },
  {
    "objectID": "experiments/sampling.html",
    "href": "experiments/sampling.html",
    "title": "Jens Müller",
    "section": "",
    "text": "from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_ollama.llms import OllamaLLM\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\nmodel = OllamaLLM(model=\"llama3.1\").bind(logprobs=True)\n\nchain = prompt | model\n\nchain.invoke({\"question\": \"What is LangChain?\"})\n\n'Let\\'s break down the concept of \"LangChain\".\\n\\nStep 1: Understanding the term \"Lang\"\\n\\nThe prefix \"Lang\" comes from the word \"Language\". So, it seems that LangChain might be related to language in some way.\\n\\nStep 2: Chain\\n\\nThe word \"Chain\" implies a connection or a link between two things. In this context, it suggests that LangChain could be a tool or a system that connects languages, concepts, or ideas in some manner.\\n\\nStep 3: Putting it together\\n\\nWith the understanding of \"Lang\" and \"Chain\", we can infer that LangChain might be a technology or platform that facilitates connections between languages, enabling multilingual interactions, language translation, or even language generation. It could also imply a tool for creating chains of reasoning or ideas across different linguistic contexts.\\n\\nStep 4: Potential applications\\n\\nConsidering the implications of LangChain as a language connection tool, potential applications could range from machine translation and text summarization to chatbots, voice assistants, and even AI-powered content creation tools.\\n\\nAm I on the right track?'\n\n\n\nmsg = model.invoke((\"human\", \"how are you today\"))\n\nmsg.response_metadata[\"logprobs\"][\"content\"][:5]\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[4], line 3\n      1 msg = model.invoke((\"human\", \"how are you today\"))\n----&gt; 3 msg.response_metadata[\"logprobs\"][\"content\"][:5]\n\nAttributeError: 'str' object has no attribute 'response_metadata'\n\n\n\n\nmsg\n\n\"Human: I'm feeling quite well, thank you for asking. Just a bit tired from the morning rush, but otherwise energized and ready to take on the day! How about you?\"\n\n\n\nimport ollama\nresponse = ollama.chat(model='llama3.1', messages=[\n  {\n    'role': 'user',\n    'content': 'Why is the sky blue?',\n  },\n],\n  n_probs=3\n)\nprint(response['message']['content'])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 2\n      1 import ollama\n----&gt; 2 response = ollama.chat(model='llama3.1', messages=[\n      3   {\n      4     'role': 'user',\n      5     'content': 'Why is the sky blue?',\n      6   },\n      7 ],\n      8   n_probs=3\n      9 )\n     10 print(response['message']['content'])\n\nTypeError: Client.chat() got an unexpected keyword argument 'n_probs'"
  },
  {
    "objectID": "experiments/test.html",
    "href": "experiments/test.html",
    "title": "Jens Müller",
    "section": "",
    "text": "from openai import OpenAI\nfrom math import exp\nimport numpy as np\nfrom IPython.display import display, HTML\nimport os\n\nopen_ai_key = \"sk-None-BcvcTP9SBTw3yuuBzCE9T3BlbkFJq0ZmKf4zLrW2tpGZKAn5\"\n#open_ai_key = \"sk-proj-nfLesDiu6oVaARYzTSOBT3BlbkFJpW0Y8kdWyLMhQKyx86Ip\"\nos.environ[\"OPENAI_API_KEY\"] = open_ai_key \n\nclient = OpenAI()\n\n\ndef get_completion(\n    messages: list[dict[str, str]],\n    model: str = \"gpt-4o-mini\",\n    max_tokens=500,\n    temperature=0,\n    stop=None,\n    seed=123,\n    tools=None,\n    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n    top_logprobs=None,\n) -&gt; str:\n    params = {\n        \"model\": model,\n        \"messages\": messages,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"stop\": stop,\n        \"seed\": seed,\n        \"logprobs\": logprobs,\n        \"top_logprobs\": top_logprobs,\n    }\n    if tools:\n        params[\"tools\"] = tools\n\n    completion = client.chat.completions.create(**params)\n    return completion\n\n\nCLASSIFICATION_PROMPT = \"\"\"You will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: {headline}\"\"\"\n\n\nheadlines = [\n    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n]\n\n\nfor headline in headlines:\n    print(f\"\\nHeadline: {headline}\")\n    API_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n        model=\"gpt-4\",\n    )\n    print(f\"Category: {API_RESPONSE.choices[0].message.content}\\n\")\n\n\n\n\nHeadline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\nCategory: Technology, as it discusses a new smartphone.\n\n\nHeadline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\nCategory: Politics, discusses a governmental initiative.\n\n\nHeadline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\nCategory: Art, involves a symphony orchestra.\n\n\n\n\nfor headline in headlines:\n    print(f\"\\nHeadline: {headline}\")\n    API_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n        model=\"gpt-4\",\n        logprobs=True,\n        top_logprobs=1,\n    )\n    top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n    html_content = \"\"\n    for i, logprob in enumerate(top_two_logprobs, start=1):\n        html_content += (\n            f\"&lt;span style='color: cyan'&gt;Output token {i}:&lt;/span&gt; {logprob.token}, \"\n            f\"&lt;span style='color: darkorange'&gt;logprobs:&lt;/span&gt; {logprob.logprob}, \"\n            f\"&lt;span style='color: magenta'&gt;linear probability:&lt;/span&gt; {np.round(np.exp(logprob.logprob)*100,2)}%&lt;br&gt;\"\n        )\n    display(HTML(html_content))\n    print(\"\\n\")\n\n\nHeadline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n\n\nOutput token 1: Explanation, logprobs: -0.93188596, linear probability: 39.38%\n\n\n\n\n\nHeadline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n\n\nOutput token 1: Politics, logprobs: -0.056269404, linear probability: 94.53%\n\n\n\n\n\nHeadline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n\n\nOutput token 1: Art, logprobs: -0.21430759, linear probability: 80.71%\n\n\n\n\n\n\n\nfor headline in headlines:\n    print(f\"\\nHeadline: {headline}\")\n    API_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n        model=\"gpt-4\",\n        logprobs=True,\n        top_logprobs=3,\n    )\n    top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n    html_content = \"\"\n    for i, logprob in enumerate(top_two_logprobs, start=1):\n        html_content += (\n            f\"&lt;span style='color: cyan'&gt;Output token {i}:&lt;/span&gt; {logprob.token}, \"\n            f\"&lt;span style='color: darkorange'&gt;logprobs:&lt;/span&gt; {logprob.logprob}, \"\n            f\"&lt;span style='color: magenta'&gt;linear probability:&lt;/span&gt; {np.round(np.exp(logprob.logprob)*100,2)}%&lt;br&gt;\"\n        )\n    display(HTML(html_content))\n    print(\"\\n\")\n\n\nHeadline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n\n\nOutput token 1: a, logprobs: -0.8605204, linear probability: 42.29%Output token 2: Technology, logprobs: -1.3646936, linear probability: 25.55%Output token 3: A, logprobs: -2.1214533, linear probability: 11.99%\n\n\n\n\n\nHeadline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n\n\nOutput token 1: Politics, logprobs: -0.078849025, linear probability: 92.42%Output token 2: a, logprobs: -3.2040284, linear probability: 4.06%Output token 3: Explanation, logprobs: -3.4173195, linear probability: 3.28%\n\n\n\n\n\nHeadline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n\n\nOutput token 1: Art, logprobs: -0.03863497, linear probability: 96.21%Output token 2: Sports, logprobs: -3.3875844, linear probability: 3.38%Output token 3: A, logprobs: -5.7630534, linear probability: 0.31%\n\n\n\n\n\n\n\nAPI_RESPONSE = get_completion(\n    [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headlines[0])}],\n    model=\"gpt-4\",\n    logprobs=True,\n    top_logprobs=3,\n)\n\n\nmessage = {\"role\": \"assistant\", \"content\": \"\"}\nmessage = CLASSIFICATION_PROMPT.format(headline=headlines[0])\nfor x in range(10):\n    API_RESPONSE = get_completion(\n        [{\"role\": \"user\", \"content\": message}],\n        model=\"gpt-4o-mini\",\n        logprobs=True,\n        top_logprobs=1,\n    )\n    message += API_RESPONSE.choices[0].logprobs.content[0].top_logprobs[0].token \n    print(message)\n    for item in API_RESPONSE.choices[0].logprobs.content[0].top_logprobs:\n        print(item.token, np.round(np.exp(item .logprob)*100,2))\n\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.a\na 99.33\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.aa\na 98.84\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.aaa\na 99.2\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.aaaa\na 99.12\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.aaaaa\na 99.39\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.aaaaaa\na 99.14\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.aaaaaaa\na 98.65\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.aaaaaaaa\na 98.73\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.aaaaaaaaa\na 98.86\nYou will be given a headline of a news article.\nClassify the article into one of the following categories: Technology, Politics, Sports, and Art.\nMAKE SURE your output is one of the four categories stated. GIve a short explanation (at most a 5 word sentence). Start with the explanation and an 'a'\nArticle headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.aaaaaaaaaa\na 97.59\n\n\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Give a fictional description of a movie in three words.\",\n    }],\n    model=\"gpt-4o-mini\",\n    logprobs=True,\n    temperature=0.2,\n    #best_of=10\n)\n\nprint(response.choices[0])\n\ncum = 0  \nfor item in response.choices[0].logprobs.content:\n    print(item.token, np.round(np.exp(item.logprob)*100,2) )\n    cum += item.logprob\n\nprint('cum', np.round(np.exp(cum)*100,8))\n\nChoice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='\"', bytes=[34], logprob=-0.7496403, top_logprobs=[]), ChatCompletionTokenLogprob(token='Time', bytes=[84, 105, 109, 101], logprob=-0.04794023, top_logprobs=[]), ChatCompletionTokenLogprob(token='-tr', bytes=[45, 116, 114], logprob=-0.45240924, top_logprobs=[]), ChatCompletionTokenLogprob(token='avel', bytes=[97, 118, 101, 108], logprob=-0.00016480287, top_logprobs=[]), ChatCompletionTokenLogprob(token='ing', bytes=[105, 110, 103], logprob=-2.1531068e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token=' love', bytes=[32, 108, 111, 118, 101], logprob=-0.9172403, top_logprobs=[]), ChatCompletionTokenLogprob(token=' triangle', bytes=[32, 116, 114, 105, 97, 110, 103, 108, 101], logprob=-0.83480483, top_logprobs=[]), ChatCompletionTokenLogprob(token='.\"', bytes=[46, 34], logprob=-0.00013548243, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='\"Time-traveling love triangle.\"', role='assistant', function_call=None, tool_calls=None, refusal=None))\n\" 47.25\nTime 95.32\n-tr 63.61\navel 99.98\ning 100.0\n love 39.96\n triangle 43.4\n.\" 99.99\ncum 4.96698725\n\n\n\nI 58.33\n appreciate 26.26\n humans 52.3\n. 99.78\n\n\n\nI 50.12\n don't 61.71\n feel 53.12\n. 99.99\ncum 16.42623173\nimport numpy as np\ncum = 0  \nfor item in response.choices[0].logprobs.content:\n    print(item.token, np.round(np.exp(item.logprob)*100,2) )\n    cum += item.logprob\n\nprint('cum', np.round(np.exp(cum)*100,8))\n\nI 54.21\n appreciate 38.8\n humanity 42.7\n. 99.75\ncum 8.95858825\n\n\n\nimport numpy as np\ncum = 0  \nfor item in response.choices[0].logprobs.content:\n    print(item.token, np.round(np.exp(item.logprob)*100,2) )\n    cum += item.logprob\n\nprint('cum', np.round(np.exp(cum)*100,8))\n\n\nimport networkx as nx\nimport plotly.graph_objects as go\nimport numpy as np\n\n# Sample data: Each path is represented by a list of (token, likelihood) tuples\n# These are example sequences; replace these with your actual sampled sequences and probabilities.\nsampled_sequences = [\n    [(\"The\", 0.9), (\"quick\", 0.8), (\"brown\", 0.7)],\n    [(\"The\", 0.9), (\"slow\", 0.6), (\"turtle\", 0.5)],\n    [(\"A\", 0.85), (\"quick\", 0.75), (\"fox\", 0.65)],\n    [(\"A\", 0.85), (\"slow\", 0.6), (\"brown\", 0.4)]\n]\n\n# Initialize the graph\nG = nx.DiGraph()\n\n# Add nodes and edges to the graph\nfor sequence in sampled_sequences:\n    current_path = \"\"\n    for i, (token, likelihood) in enumerate(sequence):\n        node_label = f\"{token} ({likelihood})\"\n        \n        # Update path and add node with hover text containing likelihood and sentence\n        current_path += f\" {token}\"\n        hover_text = f\"Token: {token}&lt;br&gt;Likelihood: {likelihood}&lt;br&gt;Path: {current_path.strip()}\"\n        \n        if i == 0:  # Add the starting node\n            G.add_node(node_label, hover=hover_text, path=current_path, likelihood=likelihood, pos=(i, len(G.nodes)))\n        else:  # Add the edge from previous to current token\n            prev_node_label = f\"{sequence[i-1][0]} ({sequence[i-1][1]})\"\n            G.add_node(node_label, hover=hover_text, path=current_path, likelihood=likelihood, pos=(i, len(G.nodes)))\n            G.add_edge(prev_node_label, node_label, weight=np.log(likelihood))\n\n# Set up left-to-right positions for nodes using their \"pos\" attribute\npos = {node: (data[\"pos\"][0], data[\"pos\"][1]) for node, data in G.nodes(data=True)}\n\n# Plotting with Plotly\nedge_x = []\nedge_y = []\nfor edge in G.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.extend([x0, x1, None])\n    edge_y.extend([y0, y1, None])\n\nedge_trace = go.Scatter(\n    x=edge_x, y=edge_y,\n    line=dict(width=0.5, color='#888'),\n    hoverinfo='none',\n    mode='lines'\n)\n\n# Create node traces with hover text for each node\nnode_x = []\nnode_y = []\nnode_text = []\nfor node in G.nodes():\n    x, y = pos[node]\n    node_x.append(x)\n    node_y.append(y)\n    node_text.append(G.nodes[node]['hover'])\n\nnode_trace = go.Scatter(\n    x=node_x, y=node_y,\n    mode='markers+text',\n    text=[node.split(\" \")[0] for node in G.nodes()],  # Show token only (without likelihood) for cleanliness\n    textposition=\"bottom center\",\n    marker=dict(size=10, color='#FFA07A'),\n    hoverinfo='text',\n    hovertext=node_text\n)\n\n# Assemble the figure with a left-to-right layout\nfig = go.Figure(data=[edge_trace, node_trace],\n                layout=go.Layout(\n                    title='Sampled Sequences Visualization (Left-to-Right)',\n                    showlegend=False,\n                    hovermode='closest',\n                    margin=dict(b=0, l=0, r=0, t=50),\n                    xaxis=dict(showgrid=False, zeroline=False, visible=False),\n                    yaxis=dict(showgrid=False, zeroline=False, visible=False)\n                ))\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Here’s a list of projects where I took the lead and played a central role in driving their development and success."
  },
  {
    "objectID": "projects/index.html#robustness-and-domain-generalization",
    "href": "projects/index.html#robustness-and-domain-generalization",
    "title": "Projects",
    "section": "Robustness and Domain Generalization",
    "text": "Robustness and Domain Generalization\n\nPhD Dissertation\nThis work represents the culmination of my research at Heidelberg University, contributing to fundamental and critical questions such as: “How and under which circumstances can we predict reliably under distribution shift?” and “How can we identify failure cases under distribution shift?”\nAdditionally, the thesis includes a comprehensive introduction to machine learning, domain generalization and causality, providing a solid foundation for the research."
  },
  {
    "objectID": "projects/index.html#finding-competence-regions-in-domain-generalization",
    "href": "projects/index.html#finding-competence-regions-in-domain-generalization",
    "title": "Projects",
    "section": "Finding Competence Regions in Domain Generalization",
    "text": "Finding Competence Regions in Domain Generalization\n\nCompetence Regions in Domain Generalization (arXiv)\nWhile I initially set out to tackle the problem of making reliable predictions when circumstances change, it became increasingly clear that this task is inherently challenging — and in many cases, impossible. This realization set my focus toward failure detection under distribution shift, addressing the question: Can we detect incorrect predictions when the underlying distribution has shifted, placing everything outside the model’s expected input space?\nIn this article, I explore the concept of competence regions, areas where a model can be considered reliable, within the context of domain generalization. This approach seeks to better understand where models perform well and where they are likely to fail under distribution shift."
  },
  {
    "objectID": "projects/index.html#understanding-context-aware-domain-generalization",
    "href": "projects/index.html#understanding-context-aware-domain-generalization",
    "title": "Projects",
    "section": "Understanding Context-Aware Domain Generalization",
    "text": "Understanding Context-Aware Domain Generalization\n\nContext Aware Domain Generalization (arXiv)\nIn my previous work on operationalizing the Principle of Independent Causal Mechanisms (ICM), I explored a method that discards information that varies across distributions to achieve robustness under distribution shift. In this article, I take the opposite approach — aiming to incorporate as much information as possible from the target distribution.\nSince labels for a novel target distribution are typically unknown, an effective strategy is to leverage additional samples from the target distribution in the form of a set. Set-encoders, a specific neural architecture designed for handling set-inputs, provide a natural solution for this task. The focus of this article was to establish and evaluate criteria that predict when this approach is likely to succeed, offering valuable insights into the applicability of set-based learning under distribution shift."
  },
  {
    "objectID": "projects/index.html#prodas-probabilistic-dataset-of-abstract-shapes",
    "href": "projects/index.html#prodas-probabilistic-dataset-of-abstract-shapes",
    "title": "Projects",
    "section": "ProDAS: Probabilistic Dataset of Abstract Shapes",
    "text": "ProDAS: Probabilistic Dataset of Abstract Shapes\n\nArticle | Repository\nIn this work, we introduce ProDAS, a dataset that allows researchers to investigate questions of disentanglement, causal relations, and more. ProDAS enables the probabilistic generation of different shapes, textures, colors, and other attributes, making it a versatile tool for studying these phenomena. It can be understood as a more advanced version of dSprites, offering richer and more complex data for experimentation."
  },
  {
    "objectID": "projects/index.html#learning-robust-models-using-the-principle-of-icm",
    "href": "projects/index.html#learning-robust-models-using-the-principle-of-icm",
    "title": "Projects",
    "section": "Learning Robust Models using the Principle of ICM",
    "text": "Learning Robust Models using the Principle of ICM\n\nArticle (arXiv) | Talk\nWhen a distribution shifts, some aspects of a system may change, while others remain unaffected. This behavior can be understood through a causal perspective, particularly via the Principle of Independent Causal Mechanisms (ICM), which suggests that different causal components of a system operate independently. Identifying the invariant components holds promise for building more robust models under distribution shift.\nIn this article, I proposed a method for identifying these components by operationalizing the Principle of ICM. The work includes both a formal mathematical framework (with proofs) and a proof-of-concept experimental analysis demonstrating the approach in practice."
  },
  {
    "objectID": "projects/index.html#legalization-a-realistic-and-better-alternative",
    "href": "projects/index.html#legalization-a-realistic-and-better-alternative",
    "title": "Projects",
    "section": "Legalization: A realistic and better alternative?",
    "text": "Legalization: A realistic and better alternative?\n\nArticle\nThis article, written during my psychology studies, reflects on the societal harm caused by the war on drugs while exploring the alternative of legalization. It combines a retrospective analysis of past consequences with evidence-based approaches for more effective future policies. (Essay is written in german)"
  },
  {
    "objectID": "blog/disc_vs_gen/index.html",
    "href": "blog/disc_vs_gen/index.html",
    "title": "The future of Work is Evaluation",
    "section": "",
    "text": "Creation and Evaluation\nThe creative process involves more than just generating ideas — it requires continuous evaluation and refinement. Take Beethoven, for example. His manuscripts reveal a constant cycle of revisions, demonstrating two key processes working in tandem: creation and critical assessment. He would repeatedly refine his compositions until his artistic vision was fully realized. This process is not limited to music or other art forms. The same dynamic is found in entrepreneurship, product development and other creative disciplines, where creation and critical evaluation work together in a continuous loop.\n\n\nAI as creator\nThis creative process mirrors the dynamic in Generative Adversarial Networks (GANs), where the generator produces a “solution” and the discriminator evaluates whether it meets the desired standard. In this context, “good enough” means that the generated output is indistinguishable from a reference dataset. The generator’s goal is to produce outputs indistinguishable from the reference, whether it is a collection of music, or other creative work.\nIn the near future, AI models will become highly proficient in both creation and refinement, increasingly capable of fulfilling a user’s intent and taking over much of the creative process. In the GAN analogy, humans will act as the discriminator, providing feedback and guiding the the AI generator throughout the generative process.\n\n\nHumans as creative directors\nHumans will take on a role similar to a creative director, guiding the direction and evaluating progress, but it will be AI that generates the music, products, or content. This shift will fundamentally change our approach to creative work. Instead of being the primary creators, we will transition to acting more like creative directors — shaping the vision while AI handles much of the execution.\nThis transformation will redefine the boundaries of human creativity. As the labor-intensive aspects of creation are taken over by AI, human efforts will focus more on providing vision, shaping direction, and refining the final outcome.\n\n\nConclusion\nIn the past, creativity was centered around craftsmanship — hands-on creation and refinement. In the near future, it will be all about creative direction. This will completely change how we work, reshaping our view of artists and creators. We may no longer think of them in terms of traditional craftsmanship, but more as creative directors who guide and orchestrate the process, while AI handles the hands-on work."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "My Blog",
    "section": "",
    "text": "Welcome to my blog! Here, I share my thoughts and explore problems that capture my attention. Topics range from machine learning and mathematics to futuristic speculations. Posts vary in length — some are concise, focusing on a single idea, while others are more comprehensive, exploring entire topics."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "My Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            December 3, 2024\n        \n        \n            LLM Sampling is Searching\n            \n            \n                \n                \n                    LLM\n                \n                \n            \n            \n            Generating samples with a Large Language Model (LLM) can be viewed as navigating a vast search space to uncover meaningful sequences. In this article, I aim to explore this search problem and offer a concise overview of some commonly used search techniques.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            October 15, 2024\n        \n        \n            The future of Work is Evaluation\n            \n            \n                \n                \n                    AI\n                \n                \n                \n                    Future\n                \n                \n            \n            \n            In a future where AI models increase in capacity, people will not generate solutions, but evaluate them and provide feedback.\n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items\n\n\n\nThe website template is adapted from Marvin Schmitt and post listing is based on the blog of Andrew Heiss. Both are shared under the CC-BY-SA 4.0 license.."
  }
]